{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1 (5 puntos)\n",
    "\n",
    "Programar el método de descenso máximo con tamaño de paso exacto para\n",
    "minimizar funciones cuadráticas:\n",
    "\n",
    "$$ f(x) = \\frac{1}{2} x^\\top \\mathbf{A} x - b^\\top x, $$\n",
    "\n",
    "donde  $\\mathbf{A} \\in  \\mathbb{R}^{n \\times n}$ y $x \\in \\mathbb{R}^n$.\n",
    "\n",
    "Dado el vector $b$,  la matriz $\\mathbf{A}$, \n",
    "un punto inicial $x_0$, un número  máximo de iteraciones $N$, \n",
    "la tolerancia $\\tau>0$. Fijar $k=0$ y repetir los siguientes pasos:\n",
    "\n",
    "1. Calcular el gradiente en el punto $x_k$, \n",
    "\n",
    "   $$g_k = \\nabla f(x_k) = \\mathbf{A} x_k - b$$\n",
    "2. Si $\\|g_k\\| < \\tau$, entonces $x_k$ es (casi) un punto estacionario.\n",
    "   Hacer $res=1$ y terminar el ciclo. \n",
    "3. Elegir la dirección de descenso como $p_k = - g_k$.\n",
    "4. Calcular el tamaño de paso $\\alpha_{k}$ que minimiza el valor de la función\n",
    "   $$\\phi_k(\\alpha) =  f(x_k + \\alpha p_k)$$\n",
    "   es decir, calcular\n",
    "   $$ \\alpha_{k} = -\\frac{ g_{k}^{\\top} p_{k}}{ p_{k}^{\\top}\\mathbf{A}p_{k}} $$\n",
    "5. Calcular el siguiente punto de la secuencia como\n",
    "   $$x_{k+1} = x_k + \\alpha_k p_k $$\n",
    "6. Si $k+1\\geq N$, hacer $res=0$ y terminar.\n",
    "7. Si no, hacer $k = k+1$ y volver el paso 1.\n",
    "6. Devolver el punto $x_k$, $f_k= \\frac{1}{2} x_k^\\top \\mathbf{A} x_k - b^\\top x_k$, $g_k$, \n",
    "   $k$ y $res$.\n",
    "\n",
    "---\n",
    "\n",
    "1. Escriba una función que implementa el algoritmo anterior \n",
    "   usando arreglos de Numpy.  \n",
    "2. Escriba una función para probar el funcionamiento del método\n",
    "   de descenso máximo. Esta función debe recibir como parámetros\n",
    "   el nombre de un archivo `.npy` que contiene las entradas de\n",
    "   una matriz cuadrada $\\mathbf{A}$, el vector $b$, un punto inicial $x_0$,\n",
    "   el número máximo de iteraciones $N$ y la\n",
    "   tolerancia $\\tau$.   \n",
    "   \n",
    "* Esta función debe crear la matriz $\\mathbf{A}$ y el vector $b$ \n",
    "  leyendo los archivos de datos.\n",
    "* Obtener el número de filas $r$ de la matriz e imprimir este valor.\n",
    "* Compruebe que la matriz es simétrica  y definida positiva calculando\n",
    "  e imprimiendo el valor $\\|\\mathbf{A} - \\mathbf{A}^\\top\\|$ y su\n",
    "  eigenvalor más pequeño (use la función `numpy.linalg.eig()`).\n",
    "* Ejecutar la función del Inciso 1.\n",
    "* Dependiendo del valor de la variable $res$, imprima un mensaje que diga que el \n",
    "  algoritmo convergió ($res=1$) o no ($res=0$).\n",
    "* Imprimir $k$, $f_k$, la norma de $g_k$, y los primeros 3 y últimos 3 elementos del arreglo $x_k$.\n",
    "* Calcule directamente el minimizador resolviendo la ecuación \n",
    "  $Ax_* = b$ e imprima el valor del error $\\|x_k - x_* \\|$.\n",
    "  \n",
    "3. Pruebe la función del Inciso 2  usando $N=1000$, la tolerancia\n",
    "   $\\tau = \\epsilon_m^{1/3}$, donde $\\epsilon_m$ es el\n",
    "   épsilon de la máquina, y los arreglos que se incluyen en \n",
    "   el archivo datosTarea04.zip, de la siguiente manera:\n",
    "\n",
    "| Matriz   | Vector     | Punto para iniciar la secuencia  |\n",
    "|----------|------------|----------------------------------|\n",
    "| A1.npy   | b1.npy     |  $x_0 = (0, -5)$                 |\n",
    "| A1.npy   | b1.npy     |  $x_0 = (7045, 7095)$            |\n",
    "| A2.npy   | b2.npy     |  $x_0 = (0,0,...,0) \\in \\mathbb{R}^{500}$             |\n",
    "| A2.npy   | b2.npy     |  $x_0 = (10000,10000,...,10000) \\in \\mathbb{R}^{500}$ |\n",
    "\n",
    "   \n",
    "## Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda declaramos el PATH de los archivos `.npy` que usaremos para probar el método de descenso máximo con paso exacto para la funciones cuadráticas de la forma\n",
    "$$ f(x) = \\frac{1}{2} x^\\top \\mathbf{A} x - b^\\top x, $$\n",
    "donde $A\\in\\mathbb{R}^{n\\times n}$ es una matriz simétrica positiva definida.\n",
    "\n",
    "Importamos el módulo `lib_t4` donde se encuentran las funciones `grad_max_quadratic` y `proof_grad_max_quadratic`, que implementan los numerales 1 y 2, respectvamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de los archivos con los datos\n",
    "data_A1='/datosTarea04/A1.npy'\n",
    "data_A2='/datosTarea04/A2.npy'\n",
    "data_b1='/datosTarea04/b1.npy'\n",
    "data_b2='/datosTarea04/b2.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función 1\n",
    "\n",
    "A continuación realizamos las pruebas correspondiente a dos diferentes condiciones iniciales $x_0$ tomando los datos de la matriz en `A1.npy` y el vector `b1.npy` para definir la función cuadrática. En cada una de las pruebas se utiliza como tolerancia $\\tau=\\epsilon_m^{1/3}$ y un número máximo de iteraciones $N=1000$, donde $\\epsilon_m$ es el épsilon de la máquina.\n",
    "\n",
    "Para $x_0=(0,-5)$ tenemos el siguiente resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas de la matriz A es 2\n",
      "||A-A.T||=  0.0\n",
      "Es una matriz simétrica\n",
      "El valor propio más pequeño es:  0.10000000000000003\n",
      "La matriz A es positiva definida\n",
      "El método de descenso máximo con paso exacto CONVERGE\n",
      "k =  69\n",
      "fk =  -62.749999999933024\n",
      "||gk|| =  4.767340294300877e-06\n",
      "xk =  [-24.49997287  25.49997743]\n",
      "||xk-x*|| =  3.528998540703229e-05\n"
     ]
    }
   ],
   "source": [
    "import lib_t4\n",
    "import importlib\n",
    "importlib.reload(lib_t4)\n",
    "from lib_t4 import *\n",
    "x0=np.array([0.0,-5.0])\n",
    "tol=np.finfo(float).eps**(1/3)\n",
    "N=1000\n",
    "proof_grad_max_quadratic(data_A1,data_b1,x0,N,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la condición inicial $x_0=(7045,7095)$ obtenemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas de la matriz A es 2\n",
      "||A-A.T||=  0.0\n",
      "Es una matriz simétrica\n",
      "El valor propio más pequeño es:  0.10000000000000003\n",
      "La matriz A es positiva definida\n",
      "El método de descenso máximo con paso exacto CONVERGE\n",
      "k =  1\n",
      "fk =  -62.75\n",
      "||gk|| =  6.425429159208664e-13\n",
      "xk =  [-24.5  25.5]\n",
      "||xk-x*|| =  9.131096203816022e-13\n"
     ]
    }
   ],
   "source": [
    "x0=np.array([7045.0,7095.0])\n",
    "proof_grad_max_quadratic(data_A1,data_b1,x0,N,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí llama la atención como en le primer paso se llegó al mínimo de la función, incluso con mayor exactitud que con la primer condición inicial como se puede constantar en el valor de $\\lVert x_k-x_\\ast\\rVert$ para cada caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función 2\n",
    "Ahora usaremos los datos de la matriz en `A2.npy` y el vector `b2.npy` para definir la función cuadrática. \n",
    "\n",
    "Ejecutando el algoritmo de descenso máximo con paso exacto para dicha función con la codición inicial $x_0=(0,0,\\dots,0)\\in\\mathbb{R}^{500}$ obtenemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas de la matriz A es 500\n",
      "||A-A.T||=  1.5063918215255853e-14\n",
      "Es una matriz simétrica\n",
      "El valor propio más pequeño es:  0.09999999999999767\n",
      "La matriz A es positiva definida\n",
      "El método de descenso máximo con paso exacto CONVERGE\n",
      "k =  332\n",
      "fk =  -5239.541412076964\n",
      "||gk|| =  5.996601797613609e-06\n",
      "Primeras 3 coordenadas de xk son:  [-11.61784712   5.44982336   0.96506345]\n",
      "Últimas 3 coordenadas de xk son:  [ -1.8852386  -10.34321936  -4.15697837]\n",
      "||xk-x*|| =  3.9297511569442596e-05\n"
     ]
    }
   ],
   "source": [
    "x0=np.zeros(500)\n",
    "proof_grad_max_quadratic(data_A2,data_b2,x0,N,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la condición inicial $x_0=(10000,10000,\\dots,10000)\\in\\mathbb{R}^{500}$ obtenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas de la matriz A es 500\n",
      "||A-A.T||=  1.5063918215255853e-14\n",
      "Es una matriz simétrica\n",
      "El valor propio más pequeño es:  0.09999999999999767\n",
      "La matriz A es positiva definida\n",
      "El método de descenso máximo con paso exacto CONVERGE\n",
      "k =  453\n",
      "fk =  -5239.541412076969\n",
      "||gk|| =  5.700360427845806e-06\n",
      "Primeras 3 coordenadas de xk son:  [-11.61784726   5.4498234    0.96506346]\n",
      "Últimas 3 coordenadas de xk son:  [ -1.8852385  -10.34321955  -4.15697837]\n",
      "||xk-x*|| =  3.859553615080582e-05\n"
     ]
    }
   ],
   "source": [
    "x0=np.full(500,10000.0)\n",
    "proof_grad_max_quadratic(data_A2,data_b2,x0,N,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de la primer función cuadrática no hay una diferencia tan radical. Con la segunda condición inicial se necesitan más iteraciones para alcanzar la convergencia que en la primero, pero al final los resultados son bastante parecidos incluso en la distancia con el punto estacionario $x_\\ast$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2 (5 puntos)\n",
    "\n",
    "Programar el método de descenso máximo con tamaño de paso seleccionado\n",
    "por la estrategia de backtracking:\n",
    "\n",
    "\n",
    "**Algoritmo de descenso máximo con backtracking:**\n",
    "\n",
    "Dada una función $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, su gradiente\n",
    "$g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, \n",
    "un punto inicial $x_0$, un número  máximo de iteraciones $N$, \n",
    "una tolerancia $\\tau>0$. Fijar $k=0$ y repetir los siguientes pasos:\n",
    "\n",
    "1. Calcular el gradiente en el punto $x_k$:\n",
    "\n",
    "   $$ g_k = \\nabla f(x_k) = g(x_k) $$\n",
    "2. Si $\\|g_k\\|<\\tau$, $x_k$ es un aproximadamente un punto estacionario, \n",
    "   por lo que hay que hacer $res=1$ y terminar el ciclo.\n",
    "3. Eligir la dirección de descenso como $p_k = - g_k$.\n",
    "4. Calcular el tamaño de paso $\\alpha_k$ mediante la estrategia de backtraking,\n",
    "   usando el algoritmo que describe más adelante.\n",
    "5. Calcular el siguiente punto de la secuencia como\n",
    "   $$x_{k+1} = x_k + \\alpha_k p_k $$\n",
    "6. Si ${k+1}>N$, hacer $res=0$ y terminar. \n",
    "7. Si no, hacer $k = k+1$ y volver el paso 1.\n",
    "8. Devolver el punto $x_k$, $f_k= f(x_k)$, $g_k$, $k$ y $res$.\n",
    "\n",
    "---\n",
    "\n",
    "** Algoritmo de backtracking **\n",
    "\n",
    "Backtracking($f$, $f_k$, $g_k$, $x_k$, $p_k$, $\\alpha_{ini}$, $\\rho$, $c$)\n",
    "\n",
    "El algoritmo recibe la función $f$, el punto $x_k$, $f_k = f(x_k)$, la dirección de descenso $p_k$,\n",
    "un valor inicial $\\alpha_{ini}$, $\\rho \\in (0,1)$, $c \\in (0,1)$.\n",
    "\n",
    "Fijar  $\\alpha = \\alpha_{ini}$  y repetir los siguientes pasos:\n",
    "\n",
    "1.  Si se cumple la condición\n",
    "    $$ f(x_k+\\alpha p_k) \\leq f_k + c \\alpha g_k^\\top p_k, $$ \n",
    "    terminar el ciclo devolviendo \n",
    "2.  Hacer $\\alpha = \\rho \\alpha$ y regresar al paso anterior.\n",
    "\n",
    "---\n",
    "\n",
    "1. Escriba una función que implementa el algoritmo de backtracking. \n",
    "2. Escriba la función que implementa el algoritmo de máximo\n",
    "   descenso con búsqueda inexacta, usando backtraking. \n",
    "   Tiene que recibir como paramétros todos los elementos que se \n",
    "   listaron para ambos algoritmos.\n",
    "3. Escriba una función para probar el funcionamiento del método\n",
    "   de descenso máximo. Esta función debe recibir la función\n",
    "   $f$, la función $g$ que devuelve su gradiente, el punto inicial $x_0$, el número  \n",
    "   máximo de iteraciones $N$, la tolerancia $\\tau>0$ y\n",
    "   el factor $\\rho$ del algoritmo de backtracking.\n",
    "   \n",
    "* Fijar los parámetros $\\alpha_{ini}=2$ y $c=0.0001$ del algoritmo de backtracking.\n",
    "* Ejecutar la función del Inciso 2.\n",
    "* Dependiendo del valor de la variable $res$, imprima un mensaje que diga que el \n",
    "  algoritmo convergió ($res=1$) o no ($res=0$)\n",
    "* Imprimir $k$, $x_k$, $f_k$ y la norma de $g_k$.\n",
    "\n",
    "4. Pruebe la función del Inciso 3  usando $N=10000$, $\\rho=0.8$, la tolerancia\n",
    "   $\\tau = \\epsilon_m^{1/3}$, donde $\\epsilon_m$ es el\n",
    "   épsilon de la máquina. Aplique esta función  a:\n",
    "* La función de Rosenbrock, descrita en la Tarea 3, usando como punto inicial $x_0= (-1.2, 1)$ y $x_0= (-12, 10)$.\n",
    "  Como referencia, el minimizador de la función  es $x_* = (1,1)$.\n",
    "  \n",
    "5. Repita el inciso anterior con $\\rho=0.5$.   \n",
    "\n",
    "   \n",
    "## Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar al ejercicio anterior, importamos el módulo `lib_t4` donde se encuentran las funciones `backtracking`, `grad_max` y `proof_grad_max` que son las implementaciones de lo que se solicita en los numerales 1,2 y 3, respectivamente.\n",
    "\n",
    "Realizamos las pruebas del método de descenso máximo con tamaño de paso obtenido a través del *algoritmo backtracking*. \n",
    "\n",
    "Fijamos un número máximo de iteraciones $N=10,000$ y una tolerancia $\\tau=\\epsilon_m^{1/3}$.\n",
    "\n",
    "\n",
    "### $\\rho=0.8$\n",
    "\n",
    "En primer lugar, probamos este método de optimización con la *Función de Rosenbrock* con la condición inicial $x_0=(-1.2,1)$ y tomando $\\rho=0.8$, el resultado es el siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking NO CONVERGE\n",
      "k =  10000\n",
      "xk =  [1.00079208 1.00158391]\n",
      "fk =  6.274640640496038e-07\n",
      "||gk|| =  0.0019653292817284995\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(lib_t4)\n",
    "from lib_t4 import *\n",
    "tol=np.finfo(float).eps**(1/3)\n",
    "N=10000\n",
    "rho=0.8\n",
    "x0=np.array([-1.2,1.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo factor $\\rho=0.8$ pero con la condición inicial $x_0=(-12,10)$ obtenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking NO CONVERGE\n",
      "k =  10000\n",
      "xk =  [ 3.92075554 15.37404809]\n",
      "fk =  8.531110164767732\n",
      "||gk|| =  2.823173435454123\n"
     ]
    }
   ],
   "source": [
    "x0=np.array([-12.0,10.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que en ambos casos se alcanza el máximo número de iteraciones. Con la condición inicial $x_0=(-12,10)$, al estar más lejos de el óptimo $x_\\ast=(1,1)$ el resultado final igual queda más lejos de $x_\\ast$ que con la primer condición inicial, pues con esta condición inicial el método de descenso máximo en las 10000 iteraciones queda mucho más cerca del valor $x_\\ast$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\rho=0.5$\n",
    "\n",
    "Ahora cambiamos  sólo el factor por el cual aumentamos la velocidad de reducción de el tamaño de paso en el *algoritmo backtracking* a $\\rho=0.5$.\n",
    "\n",
    "Para la condición inicial $x_0=(-1.2,1)$ obtenemos el siguiente resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking NO CONVERGE\n",
      "k =  10000\n",
      "xk =  [0.99998357 0.99996704]\n",
      "fk =  2.7097756887567074e-10\n",
      "||gk|| =  2.377226279771169e-05\n"
     ]
    }
   ],
   "source": [
    "rho=0.5\n",
    "x0=np.array([-1.2,1.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la condición inicial $x_0=(-12,10)$ el resultado es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking NO CONVERGE\n",
      "k =  10000\n",
      "xk =  [2.85989511 8.18266786]\n",
      "fk =  3.46055510931203\n",
      "||gk|| =  1.121519863018428\n"
     ]
    }
   ],
   "source": [
    "x0=np.array([-12.0,10.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con el factor $\\rho=0.8$ con ambas condiciones se alcanza el máximo de iteraciones, sin embargo el valor final de la función de Rosenbrock en ambos casos queda mucho más cerca del valor óptimo que es $f(x_\\ast)=0$, lo que sugiere que quizás el factor $\\rho=0.8$ en el algoritmo backtracking deja aún muy largo el tamaño de paso que puede ocasionar que rebote la búsqueda en línea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vemos que si aumentamos la velocidad a la que el tamaño de paso $\\alpha\\to 0$ tomando $\\rho=0.4$. Para la condición inicial $x_0=(-1.2,1)$ el resultado es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking CONVERGE\n",
      "k =  9989\n",
      "xk =  [1.00000616 1.00001235]\n",
      "fk =  3.80249328045783e-11\n",
      "||gk|| =  5.974110497205854e-06\n"
     ]
    }
   ],
   "source": [
    "rho=0.4\n",
    "x0=np.array([-1.2,1.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la condición inicial $x_0=(-12,10)$ los resultados son los siguientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de descenso máximo con backtracking NO CONVERGE\n",
      "k =  10000\n",
      "xk =  [1.93435739 3.74235441]\n",
      "fk =  0.873061658777709\n",
      "||gk|| =  0.6240981756328119\n"
     ]
    }
   ],
   "source": [
    "x0=np.array([-12.0,10.0])\n",
    "proof_grad_max(f_Rosenbrock,grad_Rosenbrock,x0,N,tol,rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de los otros factores $\\rho$ con $\\rho=0.4$ tenemos convergencia para la primer condición inicial. Análogamente, para la segunda condición inicial estamos más cerca del valor óptimo de la función de Rosenbrock que con los otros factores $\\rho$.\n",
    "\n",
    "Observando la gráfica de la función de Rosenbrock en la *Tarea 3*, podemos concluir que este comportamiento se puede deber a que en el rayo $\\lambda (-1.2,1.0)$ con $\\lambda\\in \\mathbb{R}$ se encuentra un precipicio algo pronunciado, entonces para pasos grandes nos podemos salir de ese valle y  alejarnos del óptimo de forma más fácil que con tamaños de paso más pequeños, por lo que eso explica que el comportamiento del método de descenso máximo mejore cuando se acelera la velocidad con la que el tamaño de paso $\\alpha\\to 0$ en el *algoritmo backtracking*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
