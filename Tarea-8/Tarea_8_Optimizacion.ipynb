{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (3 puntos)\n",
    "\n",
    "Sea $x=(x_1, x_2, ..., x_n)$ la variable independiente.\n",
    "\n",
    "Programar las siguientes funciones y sus gradientes:\n",
    "\n",
    "- Función cuadrática \n",
    "\n",
    "$$ f(\\mathbf{x}) = 0.5\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} - \\mathbf{b}^\\top\\mathbf{x}. $$\n",
    "\n",
    "Si $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$, entonces\n",
    "\n",
    "$$ \\mathbf{A} = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b} = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "\n",
    "- Función generalizada de Rosenbrock\n",
    "\n",
    "$$  f(x) = \\sum_{i=1}^{n-1} 100(x_{i+1} - x_i^2)^2 + (1 - x_{i} )^2  $$\n",
    "\n",
    "$$ x_0 = (-1.2, 1, -1.2, 1, ..., -1.2, 1) $$\n",
    "\n",
    "\n",
    "En la implementación de cada función y de su gradiente, se recibe como argumento la variable $x$\n",
    "y definimos $n$ como la longitud del arreglo $x$, y con esos datos aplicamos la \n",
    "definición correspondiente.\n",
    "\n",
    "Estas funciones van a ser usadas para probar los algoritmos de optimización.\n",
    "El  punto $x_0$ que aparece en la definición de cada función es el punto inicial\n",
    "que se sugiere para el algoritmo de optimización.\n",
    "\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importamos el módulo `lib_t8.py` que contiene la definición de las funciones anteriores y sus gradientes. Sea $f_Q$ y $f_R$ las funciones cuadráticas y generalizada de Rosenbrock respectivamente. Vamos a probar estas funciones en el punto $\\mathbf{x}=(1,1)$. \n",
    "\n",
    "Sabemos que $\\mathbf{x}$ es punto crítico de $f_R$ con óptimo $f_R(\\mathbf{x})=0$, mientras que, haciendo unos cuantos cálculos podemos ver que $f_Q(\\mathbf{x})=2$ y $\\nabla f_Q(\\mathbf{x})=(3,3)^T$. Compararemos estos valores con los resultados numéricos. \n",
    "\n",
    "En primer lugar, evaluamos en $\\mathbf{x}$ la función cuadrática $f_Q$ programada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funcion cuadrática en (1,1):  2.0\n",
      "Valor del gradiente de la funcion cuadrática en (1,1):  [[3.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "import lib_t8\n",
    "import importlib\n",
    "importlib.reload(lib_t8)\n",
    "from lib_t8 import *\n",
    "\n",
    "# Punto de prueba\n",
    "x_proof=np.array([1.0,1.0])\n",
    "\n",
    "# Implementación de la función cuadrática y su gradiente\n",
    "print('Valor de la funcion cuadrática en (1,1): ',quad_fun(x_proof))\n",
    "print('Valor del gradiente de la funcion cuadrática en (1,1): ',grad_quad_fun(x_proof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la misma prueba para la función generalizada de Rosenbrock, sabiendo que $f_R(\\mathbf{x})=0$ y $\\nabla f_R(\\mathbf{x})=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funcion generalizada de Rosenbrock en (1,1):  0.0\n",
      "Valor del gradiente de la funcion generalizada de Rosenbrock en (1,1):  [-0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#  Implementación de la función generalizada de Rosenbrock y su gradiente\n",
    "print('Valor de la funcion generalizada de Rosenbrock en (1,1): ',gen_rosenbrock(x_proof))\n",
    "print('Valor del gradiente de la funcion generalizada de Rosenbrock en (1,1): ',grad_gen_rosenbrock(x_proof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de Fletcher-Reeves:\n",
    "\n",
    "---\n",
    "\n",
    "La implementación recibe como argumentos a la función objetivo $f$, su gradiente $\\nabla f$,\n",
    "un punto inicial $x_0$, el máximo número de iteraciones $N$ y una tolerancia $\\tau>0$.\n",
    "\n",
    "1. Calcular  $\\nabla f_0 = \\nabla f(x_0)$, $p_0 = -\\nabla f_0$ y hacer $res=0$.\n",
    "2. Para $k=0,1,..., N$:\n",
    "\n",
    "- Si $\\|\\nabla f_k\\|< \\tau$, hacer $res=1$ y terminar el ciclo\n",
    "- Usando backtracking calcular el tamaño de paso  $\\alpha_k$\n",
    "- Calcular $x_{k+1} = x_k + \\alpha_k p_k$\n",
    "- Calcular $\\nabla f_{k+1} = \\nabla f(x_{k+1})$\n",
    "- Calcular \n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
    "\n",
    "- Calcular \n",
    "\n",
    "$$ p_{k+1} = -\\nabla f_{k+1} + \\beta_{k+1} p_k $$\n",
    "\n",
    "3. Devolver $x_k, \\nabla f_k, k, res$\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "1. Escriba la función que implemente el algoritmo anterior.\n",
    "2. Pruebe el algoritmo usando para cada una de las funciones del \n",
    "   Ejercicio 1, tomando el punto $x_0$ que se indica.\n",
    "3. Fije $N=50000$, $\\tau = \\epsilon_m^{1/3}$.\n",
    "4. Para cada función del Ejercicio 1 cree el punto $x_0$ correspondiente\n",
    "   usado $n=2, 10, 20$ y ejecute el algoritmo.\n",
    "   Imprima\n",
    "   \n",
    "- n,\n",
    "- f(x0),\n",
    "- las primeras y últimas 4 entradas del punto $x_k$ que devuelve el algoritmo,\n",
    "- f(xk),\n",
    "- la norma del vector $\\nabla f_k$, \n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- la variable $res$ para saber si el algoritmo puedo converger.\n",
    "  \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualizamos el módulo con el que estamos trabajando, definimos la tolerancia, el número máximo de iteraciones y el parámetro $\\rho$ del algoritmo de backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(lib_t8)\n",
    "import lib_t8\n",
    "from lib_t8 import *\n",
    "\n",
    "# Tolerancia y numero maximo de iteraciones\n",
    "tol=np.finfo(float).eps**(1/3)\n",
    "N=50000\n",
    "rho=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función cuadrática\n",
    "En primer lugar, probamos el algoritmo de Fletcher-Reeves como en clase, en este caso como es una función cuadrática es equivalente al método de gradiente conjugado. A continuación, probamos el desempeño en esta función para $n=2,10,20$\n",
    "\n",
    "Por la proposición 1 de la Clase 17, el método de gradiente conjugado en este caso es globalmente convergente y el punto óptimo es $\\mathbf{x}_\\ast=\\frac{1}{2n}(1,1,\\dots,1)^T$. Daremos cualquier punto inicial y compararemos con el óptimo correspondiente, en este caso usaremos la misma condición que se sugiere en la función generalizada de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  2\n",
      "f(x0) =  2.66\n",
      "xk =  [0.25000229 0.2499987 ]\n",
      "k =  91\n",
      "fk =  -0.24999999999256428\n",
      "||gk|| =  5.803891132856387e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  10\n",
      "f(x0) =  62.49999999999999\n",
      "Primer y últimas 4 entradas de xk =  [0.04999983 0.05000018 0.04999983 0.05000018] ... [0.04999983 0.05000018 0.04999983 0.05000018]\n",
      "k =  134\n",
      "fk =  -0.24999999999843664\n",
      "||gk|| =  5.603306622825684e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  20\n",
      "f(x0) =  248.0\n",
      "Primer y últimas 4 entradas de xk =  [0.02499995 0.02500007 0.02499995 0.02500007] ... [0.02499995 0.02500007 0.02499995 0.02500007]\n",
      "k =  138\n",
      "fk =  -0.2499999999992326\n",
      "||gk|| =  5.707639527877831e-06\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Valores de n\n",
    "n=[2,10,20]\n",
    "# Paras para cada valor de n\n",
    "for nn in n:\n",
    "    x0=np.tile([-1.2,1.0],int(nn/2))\n",
    "    proof_fletcher_reeves(quad_fun,grad_quad_fun,x0,N,tol,rho)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como usamos backtracking para estimar el óptimo del tamaño de paso no se necesariamente se obtiene la solución en $n$ pasos y en efecto hemos obtenido los vectores solución de la forma $\\frac{1}{2n}(1,1,\\dots,1)$ para $n=2,10,20$, que corresponden a los valores donde la función alcanza su óptimo en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función Generalizada de Rosenbrock\n",
    "\n",
    "Repetimos la prueba ahora con la función generalizada de Rosenbrock, sabiendo que el óptimo es $\\mathbf{x}_\\ast=(1,1,\\dots,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  2\n",
      "f(x0) =  24.199999999999996\n",
      "xk =  [0.99999539 0.99999077]\n",
      "k =  251\n",
      "fk =  2.1277093635685042e-11\n",
      "||gk|| =  4.780235391540514e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  10\n",
      "f(x0) =  2057.0\n",
      "Primer y últimas 4 entradas de xk =  [0.99999999 1.         1.         1.        ] ... [0.99999999 0.99999998 0.99999995 0.9999999 ]\n",
      "k =  2019\n",
      "fk =  2.193108694121752e-14\n",
      "||gk|| =  5.749420469861276e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Fletcher-Reeves CONVERGE\n",
      "n =  20\n",
      "f(x0) =  4598.0\n",
      "Primer y últimas 4 entradas de xk =  [1. 1. 1. 1.] ... [0.99999999 0.99999998 0.99999997 0.99999993]\n",
      "k =  38376\n",
      "fk =  1.5946132549678812e-14\n",
      "||gk|| =  6.039770533007446e-06\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paras para cada valor de n\n",
    "for nn in n:\n",
    "    x0=np.tile([-1.2,1.0],int(nn/2))\n",
    "    proof_fletcher_reeves(gen_rosenbrock,grad_gen_rosenbrock,x0,N,tol,rho)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
    "Hestenes-Stiefel:\n",
    "\n",
    "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{p_{k}^\\top\\mathbf{y}_k}  $$\n",
    "\n",
    "1. Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n",
    "2. ¿Cuál de los métodos es mejor para encontrar los óptimos de las funciones de prueba?\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos las mismas pruebas tanto para la función cuadrática como la función generalizada de Rosenbrock ahora utilizando las actualización de Hestenes-Stiefel.\n",
    "\n",
    "#### Función cuadrática\n",
    "\n",
    "En la siguiente celda presentamos el resultado de Hestenes-Stiefel para la función $f_Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  2\n",
      "f(x0) =  2.66\n",
      "xk =  [0.25000217 0.24999881]\n",
      "k =  66\n",
      "fk =  -0.24999999999340244\n",
      "||gk|| =  5.497058549749721e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  10\n",
      "f(x0) =  62.49999999999999\n",
      "Primer y últimas 4 entradas de xk =  [0.05000019 0.04999985 0.05000019 0.04999985] ... [0.05000019 0.04999985 0.05000019 0.04999985]\n",
      "k =  162\n",
      "fk =  -0.24999999999844713\n",
      "||gk|| =  5.650924470696626e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  20\n",
      "f(x0) =  248.0\n",
      "Primer y últimas 4 entradas de xk =  [0.02499993 0.02500004 0.02499993 0.02500004] ... [0.02499993 0.02500004 0.02499993 0.02500004]\n",
      "k =  227\n",
      "fk =  -0.24999999999929282\n",
      "||gk|| =  5.793279751916127e-06\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(lib_t8)\n",
    "import lib_t8\n",
    "from lib_t8 import *\n",
    "\n",
    "# Paras para cada valor de n\n",
    "for nn in n:\n",
    "    x0=np.tile([-1.2,1.0],int(nn/2))\n",
    "    proof_hestenes_stiefel(quad_fun,grad_quad_fun,x0,N,tol,rho)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función Generalizada de Rosenbrock\n",
    "\n",
    "Se puede ver que en el caso cuadrático el desempeño es el mismo porque ambas actualizaciones son equivalente al método de gradiente conjugado para el caso cuadrática. \n",
    "\n",
    "El caso de mayor interés, y en el que esperamos descrepancias es utilizando como función de prueba a la función $f_R$. \n",
    "\n",
    "A continuación, presentamos el resumen de las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  2\n",
      "f(x0) =  24.199999999999996\n",
      "xk =  [1.0000003 1.0000006]\n",
      "k =  110\n",
      "fk =  1.0800792500369892e-13\n",
      "||gk|| =  5.942872244700956e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  10\n",
      "f(x0) =  2057.0\n",
      "Primer y últimas 4 entradas de xk =  [1. 1. 1. 1.] ... [1.00000001 1.00000002 1.00000004 1.00000009]\n",
      "k =  783\n",
      "fk =  1.4272406448603126e-14\n",
      "||gk|| =  5.906937683335181e-06\n",
      "\n",
      "\n",
      "\n",
      "El algoritmo de Hestenes-Stiefel CONVERGE\n",
      "n =  20\n",
      "f(x0) =  4598.0\n",
      "Primer y últimas 4 entradas de xk =  [1. 1. 1. 1.] ... [0.99999994 0.99999988 0.99999976 0.99999953]\n",
      "k =  503\n",
      "fk =  8.817321104442124e-14\n",
      "||gk|| =  5.955100340841743e-06\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paras para cada valor de n\n",
    "for nn in n:\n",
    "    x0=np.tile([-1.2,1.0],int(nn/2))\n",
    "    proof_hestenes_stiefel(gen_rosenbrock,grad_gen_rosenbrock,x0,N,tol,rho)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando Frechet-Reeves con Hestenes-Stiefel, Frechet-Reeves tiene un mejor comportamiento en el caso de la función $f_Q$ en general, sin embargo, en el caso de la función de Rosenbrock, el comportamiento de Hestenes-Stiefel es mucho mejor que Frechet-Reeves respecto al número de iteraciones."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
