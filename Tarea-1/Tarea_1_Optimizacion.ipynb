{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (6 puntos)\n",
    "\n",
    "Programar y probar el método de la iteración de Halley para el cálculo de raíces de una función de una variable.\n",
    "\n",
    "### Descripción del método\n",
    "\n",
    "El método de Halley usa una aproximación de la función $f(x)$ de segundo orden del desarrollo de Taylor de $f(x)$. \n",
    "\n",
    "$$ f(x_{k+1}) \\approx f(x_k) +  f'(x_k) \\Delta x + \\frac{1}{2} f''(x_k) (\\Delta x)^2 $$\n",
    "\n",
    "Si igualamos a cero la aproximación tenemos que \n",
    "\n",
    "$$\n",
    "\\Delta x = - \\frac{f(x_k)}{  f'(x_k) + \\frac{1}{2} f''(x_k) \\Delta x }\n",
    "$$\n",
    "\n",
    "El valor $\\Delta x$ en el lado izquierdo de la igualdad \n",
    "corresponde a $\\Delta x = x_{k+1} - x_{k}$, mientras que el que está\n",
    "en el denominador se aproxima por el paso de Newton-Raphson:\n",
    "\n",
    "$$ \\Delta x = -\\frac{f(x_k)}{f'(x_k)}, $$ \n",
    "\n",
    "de modo que\n",
    "\n",
    "$$\n",
    "x_{k+1} - x_{k} = \n",
    "- \\frac{f(x_k)}{  f'(x_k) - \\frac{1}{2} f''(x_k)f(x_k)/f'(x_k)  },\n",
    "$$\n",
    "\n",
    "es decir, el método de Halley propone generar la secuencia\n",
    "de puntos mediante la siguiente regla:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} \n",
    "- \\frac{f(x_k)}{  f'(x_k) - \\frac{f''(x_k)f(x_k)}{2f'(x_k)}  }.\n",
    "$$\n",
    "\n",
    "1. Escriba la función que aplique el método de Halley.\n",
    "   Debe recibir como argumentos un punto inicial $x_0$, las\n",
    "   función $f(x)$, sus derivadas $f'(x)$ y $f''(x)$, \n",
    "   el número máximo de iteraciones y un tolerancia $\\tau>0$,\n",
    "   similar a la función `NewtonRaphson()` vista en el ejemplo de\n",
    "   la clase, de  modo que se detenga cuando se cumpla que $|f(x_k)|< \\tau$.\n",
    "   Defina la variable `res` que indique el resultado obtenido \n",
    "   (`res=0` se acabaron las iteraciones y no se encontró un punto\n",
    "   que satisfaga el criterio de convergencia, `res=1` el algoritmo\n",
    "   converge, `res=-1` hay un problema al evaluar la expresión.\n",
    "   La función debe devolver el último punto $x_k$, $f(x_k)$,\n",
    "   el número de iteraciones realizadas y la variable `res`.\n",
    "   \n",
    "2. Pruebe el algoritmo de Halley con las siguientes funciones y puntos inciales:\n",
    "\n",
    "$$ f_1(x) =  x^3 - 2x + 1, x_0=-1000,1000. $$ \n",
    "\n",
    "$$ f_2(x) =  1 + x - \\frac{3}{2}x^2 + \\frac{1}{6}x^3 + \\frac{1}{4}x^4, x_0=-1000,1000. $$ \n",
    "\n",
    "   En cada caso imprima $x_0$, $f(x_0)$, $x_k$, $f(x_k)$, el número de iteraciones $k$ realizadas y el valor de la variable $res$.\n",
    "\n",
    "3. Repita las pruebas anteriores con el método de Newton-Raphson\n",
    "   y escriba un comentario sobre los resultados.\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La funciones que implementan tanto el método de Halley como el algoritmo de Newton-Raphson se encuentran el el módulo `lib_t1.py` con los nombres `Halley()` y `NewtonRaphson()`. Al igual que en la clase, considerarmos a lo más $100$ iteraciones, es decir, $iterMax=100$ y una tolerancia de $\\tau=10^{-8}$ que en el código definimos como `tol=1e-8`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes celdas de código probamos el algorimo de Halley con la función $f_1$.\n",
    "\n",
    "Para el punto inicial $x_0=-1000$ tenemos el siguiente resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  -1000.0\n",
      "f(x0)  :  -999997999.0\n",
      "Raiz  :  -1.6180339887504553\n",
      "f(Raiz)  :  -3.2809310823722626e-12\n",
      "Iteraciones  :  12\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "from lib_t1 import *\n",
    "iterMax=100\n",
    "tol=1e-8\n",
    "x0=[-1000.0,1000.0]\n",
    "dic_f1_Halley=Halley(x0[0],f1,der1_f1,der2_f1,iterMax,tol)\n",
    "print_results(dic_f1_Halley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, con el punto inicial $x_0=1000$ obtenemos el siguiente resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  1000.0\n",
      "f(x0)  :  999998001.0\n",
      "Raiz  :  1.000000008768147\n",
      "f(Raiz)  :  8.768147319315744e-09\n",
      "Iteraciones  :  13\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f1_Halley=Halley(x0[1],f1,der1_f1,der2_f1,iterMax,tol)\n",
    "print_results(dic_f1_Halley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que en ambos casos obtenemos una raíz aunque estas raíces son distintas, una positiva y una negativa. Podemos ver que las raíces de $f_1$ son \n",
    "$$ x_1=1,\\ x_2=\\frac{-1-\\sqrt{5}}{2}\\approx -1.61\\text{ y }x_3=\\frac{-1+\\sqrt{5}}{2}\\approx 0.6180$$\n",
    "Con las diferences condiciones iniciales hemos encontrado $x_1$ y $x_2$, para hallar $x_3$ quízas funcione inicializar el algoritmo de Halley con un valor menor a $x_1$, por ejemplo $x_0=0.5$, eso lo probamos en la siguiente celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  0.5\n",
      "f(x0)  :  0.125\n",
      "Raiz  :  0.6180339887498948\n",
      "f(Raiz)  :  0.0\n",
      "Iteraciones  :  3\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f1_Halley=Halley(0.5,f1,der1_f1,der2_f1,iterMax,tol)\n",
    "print_results(dic_f1_Halley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "De esta forma hemos encontrado $x_3$. Ahora procedemos a hallar raíces de $f_2$ a través del algoritmo de Halley con las condiciones iniciales $x_0=-1000,1000$. Para $x_0=-1000$ tenemos el siguiente resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  -1000.0\n",
      "f(x0)  :  249831832334.33334\n",
      "Raiz  :  -2.979654185792593\n",
      "f(Raiz)  :  0.0\n",
      "Iteraciones  :  15\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f2_Halley=Halley(x0[0],f2,der1_f2,der2_f2,iterMax,tol)\n",
    "print_results(dic_f2_Halley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y para $x_0=1000$ tenemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  1000.0\n",
      "f(x0)  :  250165167667.66666\n",
      "Raiz  :  -0.546729731825445\n",
      "f(Raiz)  :  1.734723475976807e-17\n",
      "Iteraciones  :  19\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f2_Halley=Halley(x0[1],f2,der1_f2,der2_f2,iterMax,tol)\n",
    "print_results(dic_f2_Halley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Se puede ver que ambas son raíces y además son las únicas dos raíces reales de la función $f_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Repetiremos lo anterior con el método de Newton-Raphson.\n",
    "Para $f_1$ con condición inicial $x_0=-1000$ tenemos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  -1000.0\n",
      "f(x0)  :  -999997999.0\n",
      "Raiz  :  -1.6180339888222295\n",
      "f(Raiz)  :  -4.234541606251696e-10\n",
      "Iteraciones  :  20\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f1_NR=NewtonRaphson(x0[0],f1,der1_f1,iterMax,tol)\n",
    "print_results(dic_f1_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y para $x_0=1000$ obtenemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  1000.0\n",
      "f(x0)  :  999998001.0\n",
      "Raiz  :  1.0000000028195468\n",
      "f(Raiz)  :  2.8195468182445893e-09\n",
      "Iteraciones  :  22\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f1_NR=NewtonRaphson(x0[1],f1,der1_f1,iterMax,tol)\n",
    "print_results(dic_f1_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El algoritmo de Newton-Raphson encontró las mismas raíces que el algoritmo de Halley respecto a las condiciones iniciales, sin embargo, lo hizo en más iteraciones.\n",
    "Ahora probamos el método de Newton-Raphson con $f_2$ y la condición inicial $x_0=-1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  -1000.0\n",
      "f(x0)  :  249831832334.33334\n",
      "Raiz  :  -2.9796541859258454\n",
      "f(Raiz)  :  1.6091874499579717e-09\n",
      "Iteraciones  :  25\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f2_NR=NewtonRaphson(x0[0],f2,der1_f2,iterMax,tol)\n",
    "print_results(dic_f2_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "E inicializando con $x_0=1000$ tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algortimo no converge\n"
     ]
    }
   ],
   "source": [
    "dic_f2_NR=NewtonRaphson(x0[1],f2,der1_f2,iterMax,tol)\n",
    "print_results(dic_f2_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Con esta función con la condición inicial $x_0=-1000$ encontramos la mismas raíz que con el algoritmo de Halley, aunque otra vez con más iteraciones. Por otro lado, inicializando con $x_0=1000$ el algoritmo no converge, es decir, `res=0`, se alcanza el número máximo de iteraciones sin que se cumpla la condición de paro, por lo que se puede inferir que para hallar la segunda raíz de $f_2$ podría converge con un número mucho mayor de iteraciones que con las que lo hizo el algoritmo de Halley, y esto se debe a que la aproximación que usa el algoritmo de Halley es de orden mayor que la usado en Newton Raphson, y por lo tanto hay menos propagación de error en el algoritmo de Halley. Otra posibilidad es que se requiere una mejor elección de la condición inicial para el algoritmo de Newton-Raphson, si consideramos inicializar con un número mucho más cercano a la segunda raíz por la derecha que $1000$, por ejemplo $x_0=1$ como condición inicial obtenemos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo converge y los resultados son: \n",
      "x0  :  1.0\n",
      "f(x0)  :  0.9166666666666666\n",
      "Raiz  :  -0.5467297318263349\n",
      "f(Raiz)  :  -2.336894566745684e-12\n",
      "Iteraciones  :  51\n",
      "Estado  :  1\n"
     ]
    }
   ],
   "source": [
    "dic_f2_NR=NewtonRaphson(1.0,f2,der1_f2,iterMax,tol)\n",
    "print_results(dic_f2_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Por lo que también podemos inferir que el algoritmo de Newton-Raphson es mucho mas susceptible a la elección de condiciones iniciales. Y a pesar de la condición inicial seleccionada, el algoritmo de Halley es mucho mas eficiente en el número de iteraciones lo que muestra el poder de los métodos basados en una aproximación de orden mayor, con el costo de tener que obtener la segunda derivada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ejercicio 2 (4 puntos)\n",
    "\n",
    "Una manera de aproximar la función $\\cos(x)$ es mediante la función \n",
    "\n",
    "$$ C(x; n) =  \\sum_{i=0}^n c_i $$\n",
    "\n",
    "donde $n$ es un parámetro que indica la cantidad de términos en la\n",
    "suma y \n",
    "\n",
    "$$ c_i = -c_{i-1} \\frac{x^2}{2i(2i-1)} \\quad \\text{y} \\quad c_0 = 1.$$\n",
    "\n",
    "1. Programe la función $C(x;n)$.\n",
    "2. Imprima el valor del error  $C(x;n)-1$ para $x \\in \\{2\\pi, 8\\pi, 12\\pi \\}$ y\n",
    "   $n = 10, 50, 100, 200$.\n",
    "3. Imprima el valor del error  $C(x;n)+1$ para $x \\in \\{\\pi, 9\\pi, 13\\pi \\}$ y\n",
    "   $n = 10, 50, 100, 200$.\n",
    "4. Comente sobre el comportamiento de los errores obtenidos y cuál sería\n",
    "   una manera apropiada de usar esta función.\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En primer lugar, programamos esta función cuyo código fuente se encuentra en el módulo `lib_t1.py`. \n",
    "\n",
    "Para el numeral 2, imprimimos el vector $|C(x;n)-1|$ para $n\\in\\{10,50,100,200\\}$ y $x\\in\\{2\\pi,8\\pi,12\\pi\\}$, por propiedades básicas de la función coseno en cada uno de los valores $x$ se tiene $\\cos(x)=1$, por lo que $|C(x;n)-1|$ es el vector de errores que se comete para cada $x$ dado $n$. Imprimimos la evaluación de la función coseno en el vector de valores $[2\\pi,8\\pi,12\\pi]$ iterando sobre cada valor de $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para n=10 |C(x;n)-1| es igual a:  [3.01224042e-04 2.51526888e+09 1.08015596e+13]\n",
      "Para n=50 |C(x;n)-1| es igual a:  [4.66293670e-15 4.86928924e-07 1.91766915e-01]\n",
      "Para n=100 |C(x;n)-1| es igual a:  [4.66293670e-15 4.86928924e-07 1.35724564e-01]\n",
      "Para n=200 |C(x;n)-1| es igual a:  [4.66293670e-15 4.86928924e-07 1.35724564e-01]\n"
     ]
    }
   ],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "list_n=[10,50,100,200]\n",
    "values_even=np.array([2.0*mt.pi,8.0*mt.pi,12.0*mt.pi])\n",
    "for n in list_n:\n",
    "    print(f'Para n={n} |C(x;n)-1| es igual a: ',abs(pol_taylor_cos(values_even,n)-1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo el mismo ejercicio, imprimimos el vector de errores $|C(x;n)+1|$ para $n\\in\\{10,50,100,200\\}$ y $x\\in\\{3\\pi,9\\pi,13\\pi\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para n=10 |C(x;n)-1| es igual a:  [2.07517810e+00 2.90378712e+10 5.53930634e+13]\n",
      "Para n=50 |C(x;n)-1| es igual a:  [2.14495088e-13 1.09779085e-05 1.94408879e+02]\n",
      "Para n=100 |C(x;n)-1| es igual a:  [2.14495088e-13 1.09779085e-05 1.46360832e+00]\n",
      "Para n=200 |C(x;n)-1| es igual a:  [2.14495088e-13 1.09779085e-05 1.46360832e+00]\n"
     ]
    }
   ],
   "source": [
    "values_odd=np.array([3.0*mt.pi,9.0*mt.pi,13.0*mt.pi])\n",
    "for n in list_n:\n",
    "    print(f'Para n={n} |C(x;n)-1| es igual a: ',abs(pol_taylor_cos(values_odd,n)+1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que teóricamente el valor de $cos(x)$ es $1$ para todo $x\\in \\{2\\pi,8\\pi,12\\pi\\}$ y $-1$ para todo $\\{3\\pi,9\\pi,12\\pi\\}$, sin embargo, en base a los resultados anteriores la aproximación $C(x;n)$ para $n$ fijo comete mayor error cuanto más crece el valor de $x$. \n",
    "\n",
    "Para explicar esto recurrimos el **Teorema de Taylor**. Es claro que $C(x;n)$ es el polinomio de Taylor de grado $2n$ de la función coseno, luego por el Teorema de Taylor se tiene que \n",
    "$$\\cos x=C(x;n)+R_{2n}(x),$$\n",
    "donde \n",
    "$$ R_{2n}(x)=\\cos^{(2n+1)}(t)\\frac{x^{2n+1}}{(2n+1)!} \\text{ para algún }t\\in [0,x],$$\n",
    "donde $\\cos^{(2n+1)}$ representa la derivada de orden $2n+1$ de la función coseno. \n",
    "\n",
    "Por lo tanto, tenemos que el error en la aproximación tiene la forma\n",
    "$$ |\\cos x- C(x;n)|=|\\cos^{(2n+1)}(t)|\\cdot |\\frac{x^{2n+1}}{(2n+1)!}|,$$\n",
    "luego cuando $x$ se hace grande el factor $\\frac{x^{2n+1}}{(2n+1)!}$ crece exponencialmente con $n$ fijo, por tanto el error en la aproximación crece exponencialmente a medida que $x$ crece. \n",
    "\n",
    "Sin embargo, podemo reducir el factor de crecimiento del error utilizando que la función coseno es periódica. Para $x\\geq 0$ arbitrario sea\n",
    "$$ n_x=\\max\\{n\\in\\mathbb{N}\\cup\\{0\\} |\\ x-2\\pi\\cdot n>0\\}.$$\n",
    "\n",
    "Por lo tanto, si $t_x=x-2\\pi\\cdot n_x$ entonces por la definición de $n_x$ se tiene que $t_x\\in [0,2\\pi)$ y $t_x$ es el menor real positivo tal que \n",
    "$$ \\cos(x)=\\cos(t_x).$$\n",
    "\n",
    "Para $x\\leq 0$ utilizamos la paridad de la función coseno y consideramos lo anterior con $x:=-x$.\n",
    "\n",
    "Por lo que aproximamos $\\cos x$ a través de $C(t_x;n)$, y en este caso podemos acotar el error de la siguiente forma\n",
    "$$ |\\cos(x)-C(t_x;n)|\\leq |\\frac{t_x^{2n+1}}{(2n+1)!}|\\leq \\frac{(2\\pi)^{2n+1}}{(2n+1)!},$$\n",
    "y observamos que el error se va $0$ más rápido cuando $n\\to \\infty$, pues hemos acotado por un valor que ya no depende de $x$.\n",
    "\n",
    "Concluimos así que para utilizar la aproximación a $\\cos(x)$ es más conveniente usar $C(t_x;n)$ que $C(x;n)$, especialmente para $n$ pequeños pues es el caso en el que el denominador $(2n+1)!$ no tiene tanto impacto; además al utilizar $t_x$ el error se irá mucho más rápido a $0$ a medida que $n\\to\\infty$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
